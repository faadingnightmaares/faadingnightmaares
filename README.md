# faadingnightmaares

Full-stack AI engineer.

I build systems around models.

Rust, Go, Python on the backend.
TypeScript on the frontend.
Desktop AI with Tauri.
Reactive UI with React and Svelte.


## Focus

- RAG systems
- Retrieval pipelines
- Vector indexing
- Embeddings + reranking
- Inference pipelines
- Tooling around LLMs
- Local-first AI
- High-performance APIs
- Systems design


## Stack

<p align="left">
  <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/rust/rust-original.svg" width="40" height="40"/>
  <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/go/go-original.svg" width="40" height="40"/>
  <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg" width="40" height="40"/>
  <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/typescript/typescript-original.svg" width="40" height="40"/>
  <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/react/react-original.svg" width="40" height="40"/>
  <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/svelte/svelte-original.svg" width="40" height="40"/>
  <img src="https://raw.githubusercontent.com/simple-icons/simple-icons/develop/icons/tauri.svg" width="40" height="40"/>
</p>


## Systems

- RAG architecture (chunking, embeddings, indexing, reranking)
- Hybrid search (BM25 + vector)
- Async Rust/Go services
- Python model workers
- Streaming inference
- Memory-aware model loading
- Evaluation + prompt iteration


## GitHub

![GitHub Stats](https://github-readme-stats.vercel.app/api?username=faadingnightmaares&show_icons=true&theme=dark&hide_border=true&cache_seconds=1800)
